import os
import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('api_key.env')

# --- ì„¤ì • ë° ì´ˆê¸°í™” ---

@st.cache_resource
def get_gemini_client():
    """Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    try:
        # API í‚¤ê°€ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not os.getenv("GEMINI_API_KEY"):
             st.error("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
             st.stop()
        
        return genai.Client()
    except Exception as e:
        st.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.stop()
        return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜

client = get_gemini_client()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Gemini Streamlit ì±—ë´‡",
    layout="centered"
)
st.title("Gemini Streamlit ì±—ë´‡ ğŸˆ")

# ëª¨ë¸ ì„ íƒ
MODEL_NAME = "gemini-2.5-flash"

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
# Streamlitì˜ session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ ëŒ€í™” ë‚´ìš©ì„ ìœ ì§€í•©ë‹ˆë‹¤.
if "chat_session" not in st.session_state:
    # ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    st.session_state.chat_session = client.chats.create(model=MODEL_NAME)
    # ì´ˆê¸° ëŒ€í™” ë©”ì‹œì§€ ê¸°ë¡ (ì˜µì…˜)
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

# --- ëŒ€í™” ë‚´ìš© í‘œì‹œ ---

# session_stateì— ì €ì¥ëœ ëª¨ë“  ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ìƒì„± ---

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¦‰ì‹œ í‘œì‹œí•˜ê³  ê¸°ë¡ì— ì¶”ê°€
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. ì±—ë´‡ ì‘ë‹µì„ ìœ„í•œ ìë¦¬ í‘œì‹œì ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            message_placeholder = st.empty()
            full_response = ""

            # 3. Gemini API í˜¸ì¶œ ë° ìŠ¤íŠ¸ë¦¬ë°
            try:
                # ì±„íŒ… ì„¸ì…˜ì„ í†µí•´ ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ ë°›ê¸°
                response_stream = st.session_state.chat_session.send_message_stream(prompt)
                
                for chunk in response_stream:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ") # ì‘ë‹µì´ ì˜¤ëŠ” ë™ì•ˆ ì»¤ì„œì²˜ëŸ¼ í‘œì‹œ
                
                message_placeholder.markdown(full_response) # ìµœì¢… ì‘ë‹µ ì¶œë ¥

                # 4. ì±—ë´‡ì˜ ìµœì¢… ì‘ë‹µì„ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                error_message = f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})